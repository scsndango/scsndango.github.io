<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.1.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="强化学习算法       scsn_dango   [TOC]  第一部分： RL 基本概念介绍   RL 定义​    在中文维基百科中，强化学习被定义为机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期收益 wikipedia。Richard S. Sutton and Andrew G. Barto 最新的强化学习书籍《Reinforcement Learning:">
<meta property="og:type" content="article">
<meta property="og:title" content="scsndango的博客">
<meta property="og:url" content="http://yoursite.com/2019/01/25/强化学习算法/index.html">
<meta property="og:site_name" content="scsndango的博客">
<meta property="og:description" content="强化学习算法       scsn_dango   [TOC]  第一部分： RL 基本概念介绍   RL 定义​    在中文维基百科中，强化学习被定义为机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期收益 wikipedia。Richard S. Sutton and Andrew G. Barto 最新的强化学习书籍《Reinforcement Learning:">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://i.imgur.com/um0o5g6.png">
<meta property="og:image" content="https://i.imgur.com/TF83oaQ.png">
<meta property="og:image" content="https://i.imgur.com/A5r8ysO.png">
<meta property="og:updated_time" content="2019-01-25T07:53:53.270Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="scsndango的博客">
<meta name="twitter:description" content="强化学习算法       scsn_dango   [TOC]  第一部分： RL 基本概念介绍   RL 定义​    在中文维基百科中，强化学习被定义为机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期收益 wikipedia。Richard S. Sutton and Andrew G. Barto 最新的强化学习书籍《Reinforcement Learning:">
<meta name="twitter:image" content="https://i.imgur.com/um0o5g6.png">





  
  
  <link rel="canonical" href="http://yoursite.com/2019/01/25/强化学习算法/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title> | scsndango的博客</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">scsndango的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/25/强化学习算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="scsndango">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="scsndango的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-25 15:45:51 / 修改时间：15:53:53" itemprop="dateCreated datePublished" datetime="2019-01-25T15:45:51+08:00">2019-01-25</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <center> <h1>    强化学习算法    </h1> </center>
<center> <b>scsn_dango</b> </center>

<p>[TOC]</p>
<center> <h2>第一部分： RL 基本概念介绍</h2> </center>

<h3 id="RL-定义"><a href="#RL-定义" class="headerlink" title="RL 定义"></a>RL 定义</h3><p>​    在中文维基百科中，强化学习被定义为机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期收益 <a href="https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0" target="_blank" rel="noopener">wikipedia</a>。Richard S. Sutton and Andrew G. Barto 最新的强化学习书籍《Reinforcement Learning: An Introduction II》中对强化学习的定义为: Reinforcement learning is learning what to do—how to map situations to actions——so as to maximize a numerical reward signal. </p>
<h3 id="RL基本元素"><a href="#RL基本元素" class="headerlink" title="RL基本元素"></a>RL基本元素</h3><p>​    可以看出强化学习至少有这样几个基本概念： <strong>环境(Environment)、主体(Agent)、状态(State)、行动(Action)和收益(Reward)</strong> 。</p>
<div align="center">    
    <img src="https://i.imgur.com/um0o5g6.png" width="400" height="200" alt="RL" align="center">
    <center>图1</center>
</div>



<p>​    <strong>环境</strong>是一个外部系统，主体处于这个系统中，能够感知到这个系统并且能够基于感知到的状态做出一定的行动。比如在 MR(Montezuma’s Revenge) 中，环境就是80x80像素大小的游戏界面。</p>
<p>​    <strong>主体</strong>是一个嵌入到环境中的系统，能够通过采取行动来改变环境的状态。比如在MR中，主体就是玩家操控的小人，小人能够根据当前环境的状态做出一个动作（上下左右移动或者跳跃），从而改变环境的状态。</p>
<p>​    <strong>状态</strong>是指当前环境的一个时间切片。在MR中就是一张特定时间的80x80大小的图片。</p>
<p>​    <strong>行动</strong>是指主体做出的行为。在MR中指上下左右、跳跃的操作。</p>
<p>​    <strong>收益</strong>是一个标量，指的是环境对当前动作或者状态的一个奖励。在MR中指的是系统定义的一个收益，既可以是在游戏回合结束的时候给的 <em>Game Over</em> 或者 <em>Win</em> 这样的全局收益，也可以是一个局部收益，比如拿到 <em>钥匙</em> 或者去到另一个 <em>房间</em>。 </p>
<h3 id="RL与其他机器学习的关系"><a href="#RL与其他机器学习的关系" class="headerlink" title="RL与其他机器学习的关系"></a>RL与其他机器学习的关系</h3><p>​    RL和传统的机器学习(监督学习 Supervised Learning，非监督学习 Unsupervised Learning，半监督学习 Semi-Supervised Learning)既有一定的联系，也存在很大的区别。大致的包含关系如图2所示。</p>
<div align="center">    
    <img src="https://i.imgur.com/TF83oaQ.png" width="300" height="300" alt="RL and ML" align="center">
    <center>图2</center>
</div>


<p>​    强化学习主要有以下几个特点：</p>
<p>​    <strong>1. 试错学习</strong>：强化学习一般没有直接的指导信息，Agent 要以不断与 Environment 进行交互，通过试错的方式来获得最佳策略(Policy)。</p>
<p>​    <strong>2. 延迟回报</strong>：强化学习的指导信息很少，而且往往是在事后（最后一个状态(State)）才给出的。比如 MR 中可能只有在每一次游戏结束以后才有一个 <em>Game Over</em> 或者 <em>Win</em> 的回报。</p>
<p>​    总的来说，RL与其他机器学习算法不同的地方在于：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 没有监督者，只有一个Reward信号；</span><br><span class="line">2. 反馈是延迟的，不是立即生成的；</span><br><span class="line">3. 强化学习是序列学习，时间在强化学习中具有重要的意义；</span><br><span class="line">4. Agent的行为会影响以后所有的决策。</span><br></pre></td></tr></table></figure>
<p>​    RL可以被抽象为一个序列预测的问题，只不过序列是通过类似图灵机一样的原理产生的，后一个State只有在前一个Action做出以后才可以得到。</p>
<script type="math/tex; mode=display">
S_0\stackrel{a_0}{\longrightarrow}S_1\stackrel{a_1}{\longrightarrow}...\stackrel{a_{n-1}}{\longrightarrow}S_n</script><p>​    其中$S_i$表示i时刻的State，$a_i$表示i时刻的Action。RL学习的目标就是学习一个根据当前State选择一个能够最大化全局收益的Action，我们把Agent根据State选择Action的方法叫做策略(Policy)。</p>
<center> <h2>    第二部分：RL 算法    </h2></center>

<p>​    强化学习的算法主要分为两大类： <strong>基于值的算法(Value-Based)</strong> 和 <strong>基于策略的算法(Policy-Based)</strong>。我首先分别介绍一下基于值和基于策略的经典算法，然后介绍一个将基于值和基于策略的算法的优点结合起来的框架——Actor-Critic(AC)框架。在AC框架下进一步介绍目前学术界用得最多的几种强化学习算法，也包括《RND》这篇论文中使用的PPO算法。</p>
<h3 id="基于值的算法"><a href="#基于值的算法" class="headerlink" title="基于值的算法"></a>基于值的算法</h3><p>​    在介绍基于值的算法之前首先介绍两个概念 <strong>状态价值函数(State Value Function)-V(s)</strong> 和 <strong>行为价值函数(Quality of State-Action function)-Q(s,a)</strong>。</p>
<p>​    <strong>状态价值函数</strong>：状态价值函数V(s)，输入是一个状态，输出是该状态的预期Reward。  </p>
<script type="math/tex; mode=display">
V_{\pi}(s) = E_{\pi}[G_0 | S_0 = s]</script><p>​    其中$\pi$表示Agent选择Action的策略的概率分布， $G_0|S_0=s$表示从状态s开始到$G_0$状态整个序列。所以$V_{\pi}(s)$表示从当前状态开始到达$G_0$状态的预期收益。</p>
<p>​    特别地，如果我们用$R_t$表示t时刻的预期收益，那么有</p>
<script type="math/tex; mode=display">
V_{\pi}(s) = E_{\pi}[G_0 | S_0 = s] = E_{\pi}[\sum_{t=0}^{\infty}{\gamma^{t}R_{t+1}|S_0=s}]</script><p>​    其中$\gamma$表示折扣因子，体现与当前状态更近的状态对与当前状态的预期期望贡献更大。</p>
<p>​    <strong>行为价值函数</strong>：行为价值函数Q(s,a)，输入是一个状态和一个行动，输出是在该状态下采取该行动的预期收益，那么有</p>
<script type="math/tex; mode=display">
Q_{\pi}(s, a) = E_{\pi}[G_0 | S_0 = s, A_0 = a] = E_{\pi}[\sum_{t=0}^{\infty}{\gamma^{t}R_{t+1}|S_0 = s, A_0 = a}]</script><p>​    易知，V(s)和Q(s,a)之间有这样的关系</p>
<script type="math/tex; mode=display">
V_{\pi}(s) = \sum_{a \in A}{Q_{\pi}(s, a)}</script><h4 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h4><p>​    下面我们给出经典的Q-learning的算法，伪代码如下所示</p>
<div align="center">    
    <img src="https://i.imgur.com/A5r8ysO.png" width="600" height="300" alt="Q-learning pseudocode" align="center">
</div>

<p>​    Q-learning 算法通过构建和维护一个Q表，Q表中的每一项表示Q(s,a)，来找到一个最优策略，这个策略能够最大化从当前状态开始所有的后继行动的期望收益。</p>
<p>​    Q-learning最重要的部分在于对于Q值的更新，从伪代码中我们可以看到，对于Q值的更新$\Delta Q$是两部分的差值乘以系数$\alpha$。一部分是$r+\gamma max_{a^{\prime}}Q(s^{\prime}, a^{\prime})$表示当前环境给出的即时回报，r表示当前环境给出的即时回报，$\gamma max_{a^{\prime}}Q(s^{\prime}, a^{\prime})$是对是对$Q(s^{\prime}, a^{\prime})$的最大估计（折扣因子为的最大估计（折扣因子为$\gamma$），所以第一部分总的表示对于当前(s,a)的Q值的现实值；另一部分为Q(s,a)表示Q(s,a)的估计值。</p>
<p>​    除了Q-learning以外，还有Deep Q-learning、Double Q-learning 和 SARSA等基于值的算法。一般来说基于值的算法都是先评估每个<em>(s, a)</em> 元组的Q值-Q(s,a)，再根据Q值求最优策略，基于值的方法适用于比较简单（状态空间比较小，或者Action数目较小）的问题，它有较高的数据利用率并且能稳定收敛。</p>
<p>​    对于Q-learning来说，因为需要构建一个Q表，每一个(s,a)元组都需要对应一个Q值，所以只能解决State和Action均可数并且数目较小的问题。Deep Q-learning通过深度神经网络(Deep Neural Network, DNN)来估计一个函数$g: S{\rightarrow}R^{|A|}$用于对每一个State s，计算一个$|A|$维的向量，向量的每一维表示Q(s,a)对应的值，这样就能够应对State数目无穷的情况，但是仍然没办法解决$|A|{\rightarrow}{\infty}$的情况。</p>
<h3 id="基于策略的算法"><a href="#基于策略的算法" class="headerlink" title="基于策略的算法"></a>基于策略的算法</h3><p>​    我们已经知道Q-learning、DQN等基于价值的方法通过计算每一个状态动作的价值，选择价值最大的动作执行。这是一种间接选择策略的做法，并且几乎没办法处理Action数目无穷的情况。那么我们能不能直接对策略进行建模呢？</p>
<p>​    一种比较直观的想法是我们可以构建这样一个策略网络(Policy Network) $PN: S {\rightarrow} A$，输入一个状态直接输出对应的Action，而不是得到一个状态价值V(s)或者每个Action对应的Q值Q(s, a)，然后直接对这个策略网络进行更新，从而直接对策略选择建模。如果我们用神经网络来模拟$PN$，那么可以形式化的表示为：</p>
<script type="math/tex; mode=display">
a = \pi(s, \theta)\ or\ a = \pi(a|s, \theta)</script><p>​    可以直接输出确定的Action，也可以输出Action的一个概率分布。在输出概率分布的时候，虽然形式上和DQN类似都是$S{\rightarrow}R^{|A|}$，但是DQN输出的是Q值，并且是基于Q值做Action的决策，而$PN$直接得到的是Action的概率分布，并且对于$|A|{\rightarrow} {\infty}$，$PN$能够直接预测出Action。</p>
<h4 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h4><p>​    $Policy\ Gradient$是基于策略的算法中最基础的一种算法。通过对收益期望求梯度，从而对Policy Network的参数进行更新。</p>
<p>​    定义收益期望$J(\theta)$如下：</p>
<script type="math/tex; mode=display">
J(\theta) = E_{\tau{\sim}\pi_{\theta}(\tau)}[r(\tau)] = \int_{\tau\sim\pi(\tau)}r(\tau)\pi_{\theta}(\tau)d\tau</script><script type="math/tex; mode=display">
\theta^{*} = \mathop{argmax}_{\theta}(J(\theta))</script><p>​    对$J(\theta)$求导有</p>
<script type="math/tex; mode=display">
\bigtriangledown_{\theta}J(\theta) = \bigtriangledown_{\theta}\int_{\tau\sim\pi(\tau)}r(\tau)\pi_{\theta}(\tau)d\tau=\int_{\tau\sim\pi(\tau)}r(\tau)\bigtriangledown_{\theta}\pi_{\theta}(\tau)d\tau</script><p>​    又因为</p>
<script type="math/tex; mode=display">
\bigtriangledown_{\theta}\pi_{\theta}(\tau) = \pi_{\theta}(\tau) \frac{\bigtriangledown_{\theta}\pi_{\theta}(\tau)}{\pi_{\theta}(\tau)} = \pi_{\theta}(\tau)\bigtriangledown_{\theta}log\pi_{\theta}(\tau)</script><script type="math/tex; mode=display">
\begin{align}
\bigtriangledown_{\theta}J(\theta) 
&= \int_{\tau\sim\pi(\tau)}\pi_{\theta}(\tau)r(\tau)\bigtriangledown_{\theta}log\pi_{\theta}(\tau)d\tau 
\\&= E_{\tau{\sim}\pi_{\theta}(\tau)}[r(\tau)\bigtriangledown_{\theta}log\pi_{\theta}(\tau)]
\end{align}</script><script type="math/tex; mode=display">
\begin{align}
log\pi_{\theta}(\tau) 
& = log\pi_{\theta}(s_1, a_1, s_2, a_2, ...s_T, a_T) 
\\&= log\{p(s_1)\prod_{t=1}^{T}[\pi_{\theta}(a_t|s_t)p(s_{t+1}|s_t, a_t)]\}
\\&= logp(s_1)+\sum_{t=1}^{T}log\pi_{\theta}(a_t|s_t)+\sum_{t=1}^{T}logp(s_{t+1}|s_t, a_t)
\\&= logp(s_T)+\sum_{t=1}^{T}log\pi_{\theta}(a_t|s_t) = \sum_{t=1}^{T}log\pi_{\theta}(a_t|s_t)
\end{align}</script><script type="math/tex; mode=display">
r(\tau) = \sum_{t=1}^{T}r(s_t, a_t)</script><script type="math/tex; mode=display">
\bigtriangledown_{\theta}J(\theta) 
= E_{\tau{\sim}\pi_{\theta}(\tau)}[\sum_{t=1}^{T}\bigtriangledown_{\theta} log\pi_{\theta}(a_t|s_t)\sum_{t=1}^{T}r(s_t, a_t)]</script><p>​    最终我们得到了一个漂亮的$\bigtriangledown_{\theta}J(\theta)$的表达式，期望里面包括两个部分$\sum_{t=1}^{T}\bigtriangledown_{\theta} log\pi_{\theta}(a_t|s_t)$表示的是获取当前Trace的概率的梯度，$\sum_{t=1}^{T}r(s_t, a_t)$表示的是当前路径的总的回报。因为回报是一个总的回报，只能在一个轮次之后才能得到，所以Policy Gradient算法只能针对每一轮次更新，无法针对每个step更新。</p>
<p>​    一个Policy Gradient算法REINFORCE的伪代码如下:<br>​    $1.\ sample {\tau^i} \ from\ \pi_{\theta}(a_t|s_t)\ (run\ the\ policy)$<br>​    $2.\ \bigtriangledown_{\theta}J(\theta) \approx \sum_i(\sum_{t=1}^{T}\bigtriangledown_{\theta} log\pi_{\theta}(a_t^i|s_t^i)\sum_{t=1}^{T}r(s_t^i, a_t^i))$<br>​    $3.\ \theta \leftarrow \theta + \alpha \bigtriangledown_{\theta}J(\theta)​$</p>
<h3 id="Actor-Critic-框架"><a href="#Actor-Critic-框架" class="headerlink" title="Actor-Critic 框架"></a>Actor-Critic 框架</h3><h4 id="Based-Actor-Critic"><a href="#Based-Actor-Critic" class="headerlink" title="Based Actor-Critic"></a>Based Actor-Critic</h4><p>​    由于最基础的Policy Gradient算法只能实现每轮次更新，很难准确地把Reward反馈回去，训练效率很差，并且很容易不收敛。所以想要将$\sum_{t=1}^{T}r(s_t^i, a_t^i)$ 替换为$Q(s_t^i, a_t^i)$使用价值函数对当前的$(s_t^i, a_t^i)$二元组的期望收益做一个评估，这样就能在每一步获取$\bigtriangledown_{\theta} log\pi_{\theta}(a_t^i|s_t^i)Q(s_t^i, a_t^i)$从而更新参数。</p>
<p>​    所以最基础的AC框架的期望收益函数$J(\theta)$的梯度有如下的形式：</p>
<script type="math/tex; mode=display">
\bigtriangledown_{\theta}J(\theta) 
= E_{\tau{\sim}\pi_{\theta}(\tau)}[\sum_{t=1}^{T}\bigtriangledown_{\theta} log\pi_{\theta}(a_t|s_t)Q(s_t, a_t)]</script><h4 id="Advantage-Actor-Critic-A2C"><a href="#Advantage-Actor-Critic-A2C" class="headerlink" title="Advantage Actor Critic(A2C)"></a>Advantage Actor Critic(A2C)</h4><p>​    后来研究表明这样的形式计算$Q(s_t, a_t)$有很大的方差。为了减小方差，将$Q(s_t, a_t)$替换为$Q(s_t, a_t) - V(s_t)$，又结合$Q(s,a)$和$V(s)$之间的关系（前文有过相关讨论），得到了一个Advantage函数，形式如下：</p>
<script type="math/tex; mode=display">
A^{\pi}(s_t, a_t) = r(s_t, a_t) + V^{\pi}(s_{t+1}) - V^{\pi}(s_t)</script><p>​    所以想要求得$A^{\pi}(s_t, a_t)$的值，我们只需要用一个神经网络对$V(s_t)$建模就好了。伪代码如下：</p>
<p>​    $batch\ actor\ critic\ algorithm$<br>​    $1.\ sample\ {s_i, a_i}\ from\ \pi_{\theta}(a|s)\ (run\ it\ on\ the\ robot)$<br>​    $2.\ fit\ \hat{V}_{\Phi}^{\pi}(s)\ to\ sampled\ reward\ sums$<br>​    $3.\ evaluate\ \hat{A}^{\pi}(s_i, a_i) =r(s_i, a_i) + \hat{V}_{\Phi}^{\pi}(s_{i}^{\prime}) - \hat{V}_{\Phi}^{\pi}(s_i)$<br>​    $4.\  \bigtriangledown_{\theta}J(\theta) = \sum_i \bigtriangledown_{\theta} log\pi_{\theta}(a_i|s_i)\hat{A}^{\pi}(s_i, a_i)$<br>​    $5.\ \theta \leftarrow \theta + \alpha \bigtriangledown_{\theta}J(\theta)$</p>
<h4 id="Trust-Region-Policy-Optimization-TRPO"><a href="#Trust-Region-Policy-Optimization-TRPO" class="headerlink" title="Trust Region Policy Optimization (TRPO)"></a>Trust Region Policy Optimization (TRPO)</h4><p>​    虽然A2C很好的把Policy-Based和Value-Based两种方法结合了起来，并且能够做到step级别的更新，但是A2C没有考虑这样的问题：每一次的更新是否能够保证新的策略的$J_{new}(\theta)$大于$J_{old}(\theta)$。</p>
<p>​    Schulman 2015年发表在ICML的论文《Trust Region Policy Optimization》讨论了这个问题，并且提出了TRPO算法，从理论上能够证明$J_{new}(\theta) \ge J_{old}(\theta)$ 。Schulman把最终的优化问题转换成了</p>
<script type="math/tex; mode=display">
\theta_{k+1} = \mathop{argmax}_\theta L(\theta_{k}, \theta)
\\ s.t.\ \bar{D}_{KL}(\theta || \theta_{k}) \le \delta
\\ where\ L(\theta_{k}, \theta) = \mathop{E}_{s,a\sim \pi_{\theta_{k}}}[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{k}}(a|s)}A^{\pi_{\theta_k}}(s,a)]</script><p>​    利用$KL$距离来限制old policy和new policy之间的距离，并且修改了目标函数，使得在满足$KL$限制下，$J_{new}(\theta) \ge J_{old}(\theta)$。</p>
<p>​    TRPO在理论上和实践中都有很好的效果。</p>
<h4 id="Proximal-Policy-Optimization-PPO"><a href="#Proximal-Policy-Optimization-PPO" class="headerlink" title="Proximal Policy Optimization(PPO)"></a>Proximal Policy Optimization(PPO)</h4><p>​    TRPO虽然在理论上和实践中都有很好的效果，但是因为最后求解的问题过于复杂，导致训练时间复杂度很高。为了减少时间上的开销，OpenAI又提出了一个TRPO的改进方法PPO，通过一个Clip函数来截断$r_t(\theta)$，从而用很小的代价实现了和$KL$距离的限制条件类似的功能。新的目标函数为：</p>
<script type="math/tex; mode=display">
L^{CLIP}(\theta) = \hat{E}_t[\min(r_t(\theta)\hat{A}_t), \mathop{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t]
\\ r_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{k}}(a_t|s_t)}</script><p><strong>Reference</strong></p>
<p>[1] Burda Y, Edwards H, Storkey A, et al. Exploration by Random Network Distillation[J]. arXiv preprint arXiv:1810.12894, 2018.</p>
<p>[2] Schulman J, Wolski F, Dhariwal P, et al. Proximal policy optimization algorithms[J]. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>[3] Schulman J, Levine S, Abbeel P, et al. Trust region policy optimization[C]//International Conference on Machine Learning. 2015: 1889-1897.</p>
<p>[4] Sutton R S, McAllester D A, Singh S P, et al. Policy gradient methods for reinforcement learning with function approximation[C]//Advances in neural information processing systems. 2000: 1057-1063.</p>
<p>[5] Sutton R S, Barto A G. Reinforcement learning: An introduction[M]. MIT press, 2018.</p>
<p>[6] Watkins C J C H, Dayan P. Q-learning[J]. Machine learning, 1992, 8(3-4): 279-292.</p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/01/07/线性代数基础知识/" rel="next" title>
                <i class="fa fa-chevron-left"></i> 
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/02/Sampling Methods/" rel="prev" title>
                 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">scsndango</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">25</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">1.</span> <span class="nav-text">    强化学习算法    </span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">1.1.</span> <span class="nav-text">第一部分： RL 基本概念介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RL-定义"><span class="nav-number">1.1.1.</span> <span class="nav-text">RL 定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RL基本元素"><span class="nav-number">1.1.2.</span> <span class="nav-text">RL基本元素</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RL与其他机器学习的关系"><span class="nav-number">1.1.3.</span> <span class="nav-text">RL与其他机器学习的关系</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">1.2.</span> <span class="nav-text">    第二部分：RL 算法    </span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基于值的算法"><span class="nav-number">1.2.1.</span> <span class="nav-text">基于值的算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Q-learning"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">Q-learning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于策略的算法"><span class="nav-number">1.2.2.</span> <span class="nav-text">基于策略的算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Policy-Gradient"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">Policy Gradient</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Actor-Critic-框架"><span class="nav-number">1.2.3.</span> <span class="nav-text">Actor-Critic 框架</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Based-Actor-Critic"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">Based Actor-Critic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Advantage-Actor-Critic-A2C"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">Advantage Actor Critic(A2C)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Trust-Region-Policy-Optimization-TRPO"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">Trust Region Policy Optimization (TRPO)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Proximal-Policy-Optimization-PPO"><span class="nav-number">1.2.3.4.</span> <span class="nav-text">Proximal Policy Optimization(PPO)</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">scsndango</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.1.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.0"></script>



  
  <script src="/js/scrollspy.js?v=7.1.0"></script>
<script src="/js/post-details.js?v=7.1.0"></script>



  


  <script src="/js/next-boot.js?v=7.1.0"></script>


  

  

  

  


  


  




  

  

  
  

  
  

  


  

  

  

  

  

  

  

  
  
  
  <script src="/lib/bookmark/bookmark.min.js?v=1.0"></script>
  <script>
  
    bookmark.scrollToMark('auto', "#更多");
  
  </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  

  

  

</body>
</html>
